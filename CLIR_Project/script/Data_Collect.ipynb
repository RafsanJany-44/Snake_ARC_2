{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27ab0361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing RSS feeds: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 100 new documents. Total: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import feedparser\n",
    "import json\n",
    "import os\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import html\n",
    "import re\n",
    "\n",
    "def html_to_text(s):\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = html.unescape(s)\n",
    "    soup = BeautifulSoup(s, \"html.parser\")\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def collect_from_rss_feeds(rss_feeds, max_docs=200, doc_prefix=\"en\"):\n",
    "    docs = []\n",
    "    seen_urls = set()\n",
    "    doc_i = 0\n",
    "\n",
    "    for rss_url in tqdm(rss_feeds, desc=\"Processing RSS feeds\"):\n",
    "        feed = feedparser.parse(rss_url)\n",
    "\n",
    "        for entry in feed.entries:\n",
    "            url = getattr(entry, \"link\", \"\").strip()\n",
    "            if not url or url in seen_urls:\n",
    "                continue\n",
    "\n",
    "            title = getattr(entry, \"title\", \"\").strip()\n",
    "            summary_raw = getattr(entry, \"summary\", \"\").strip()\n",
    "            summary_text = html_to_text(summary_raw)\n",
    "\n",
    "            date = getattr(entry, \"published\", \"\")\n",
    "\n",
    "            seen_urls.add(url)\n",
    "\n",
    "            docs.append({\n",
    "                \"doc_id\": f\"{doc_prefix}_{doc_i:06d}\",\n",
    "                \"title\": html_to_text(title),\n",
    "                \"body\": summary_text,\n",
    "                \"url\": url,\n",
    "                \"date\": date,\n",
    "                \"language\": doc_prefix,\n",
    "                \"token_count\": len(summary_text.split())\n",
    "            })\n",
    "\n",
    "            doc_i += 1\n",
    "\n",
    "            if len(docs) >= max_docs:\n",
    "                return docs\n",
    "\n",
    "    return docs\n",
    "\n",
    "rss_feeds=[\n",
    "    \"https://www.dhakatribune.com/feed/business\"\n",
    "     \n",
    "]\n",
    "\n",
    "\n",
    "saving_path = r\"C:\\Users\\RAZER\\Documents\\GitHub\\Snake_ARC_2\\CLIR_Project\\data2\"\n",
    "file_path = os.path.join(saving_path, \"document_en_big.json\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        existing_docs = json.load(f)\n",
    "else:\n",
    "    existing_docs = []\n",
    "\n",
    "existing_urls = {doc[\"url\"] for doc in existing_docs}\n",
    "\n",
    "new_docs = collect_from_rss_feeds(rss_feeds)\n",
    "new_docs = [doc for doc in new_docs if doc[\"url\"] not in existing_urls]\n",
    "\n",
    "start_id = len(existing_docs)\n",
    "for i, doc in enumerate(new_docs):\n",
    "    doc[\"doc_id\"] = f\"en_{start_id + i:06d}\"\n",
    "\n",
    "all_docs = existing_docs + new_docs\n",
    "\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_docs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Added {len(new_docs)} new documents. Total: {len(all_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "960c3566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: 190\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(r\"C:\\Users\\RAZER\\Documents\\GitHub\\Snake_ARC_2\\CLIR_Project\\data2\\document_en_big.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    docs_en = json.load(f)\n",
    "\n",
    "print(\"Before filtering:\", len(docs_en))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d5f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efaa124a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing RSS feeds:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing RSS feeds: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 new documents. Total: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import feedparser\n",
    "import json\n",
    "import os\n",
    "\n",
    "import requests # Make sure to import this\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def collect_from_rss_feeds(rss_feeds, max_docs=200):\n",
    "    docs = []\n",
    "    seen_urls = set()\n",
    "    doc_i = 0\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    for rss_url in tqdm(rss_feeds, desc=\"Processing RSS feeds\"):\n",
    "        try:\n",
    "            response = requests.get(rss_url, headers=headers, timeout=15)\n",
    "            feed = feedparser.parse(response.content)\n",
    "            \n",
    "            for entry in feed.entries:\n",
    "                url = getattr(entry, \"link\", \"\").strip()\n",
    "                if not url or url in seen_urls:\n",
    "                    continue\n",
    "                \n",
    "                title = getattr(entry, \"title\", \"\").strip()\n",
    "                raw_body = getattr(entry, \"summary\", \"\").strip()\n",
    "                \n",
    "                # --- CLEANING STEP ---\n",
    "                # This removes all the <div>, <p>, etc., and returns clean text\n",
    "                soup = BeautifulSoup(raw_body, \"html.parser\")\n",
    "                clean_body = soup.get_text(separator=\" \", strip=True)\n",
    "                \n",
    "                date = getattr(entry, \"published\", \"\")\n",
    "                \n",
    "                seen_urls.add(url)\n",
    "                docs.append({\n",
    "                    \"doc_id\": f\"en_{doc_i:06d}\",\n",
    "                    \"title\": title,\n",
    "                    \"body\": clean_body, # Cleaned text\n",
    "                    \"url\": url,\n",
    "                    \"date\": date,\n",
    "                    \"language\": \"en\",\n",
    "                    \"token_count\": len(clean_body.split())\n",
    "                })\n",
    "                doc_i += 1\n",
    "                if len(docs) >= max_docs:\n",
    "                    return docs\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {rss_url}: {e}\")\n",
    "            \n",
    "    return docs\n",
    "\n",
    "rss_feeds = [\n",
    "    # --- ALREADY WORKING FOR YOU ---\n",
    "    \"https://www.tbsnews.net/rss.xml\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# The rest of your code remains the same...\n",
    "\n",
    "\n",
    "\n",
    "saving_path = r\"C:\\Users\\RAZER\\Documents\\GitHub\\Snake_ARC_2\\CLIR_Project\\data2\"\n",
    "file_path = os.path.join(saving_path, \"document_en.json\")\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        existing_docs = json.load(f)\n",
    "else:\n",
    "    existing_docs = []\n",
    "\n",
    "existing_urls = {doc[\"url\"] for doc in existing_docs}\n",
    "\n",
    "new_docs = collect_from_rss_feeds(rss_feeds)\n",
    "new_docs = [doc for doc in new_docs if doc[\"url\"] not in existing_urls]\n",
    "\n",
    "start_id = len(existing_docs)\n",
    "for i, doc in enumerate(new_docs):\n",
    "    doc[\"doc_id\"] = f\"en_{start_id + i:06d}\"\n",
    "\n",
    "all_docs = existing_docs + new_docs\n",
    "\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_docs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Added {len(new_docs)} new documents. Total: {len(all_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d1748",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_feeds = [\n",
    "    # --- ALREADY WORKING FOR YOU ---\n",
    "    \"https://www.tbsnews.net/rss.xml\",\n",
    "    \"https://en.prothomalo.com/feed\",\n",
    "\n",
    "    # --- NEW VERIFIED SOURCES (2025) ---\n",
    "    # 1. The Daily Messenger (Very active and standard-compliant)\n",
    "    \"https://www.dailymessenger.net/rss.xml\",\n",
    "    \n",
    "    # 2. Daily Sun (Section-specific feeds are currently more stable)\n",
    "    \"https://www.daily-sun.com/rss/national\",\n",
    "    \"https://www.daily-sun.com/rss/business\",\n",
    "    \n",
    "    # 3. Bangladesh Sangbad Sangstha (BSS - The National News Agency)\n",
    "    \"https://www.bssnews.net/rss/rss.xml\",\n",
    "    \n",
    "    # 4. Financial Express (Focuses on Economy/Business)\n",
    "    \"https://thefinancialexpress.com.bd/rss/national\",\n",
    "    \"https://thefinancialexpress.com.bd/rss/trade\"\n",
    "]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09181de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
